+++
abstract = "Different components in a machine learning system - for example the choice of  model architecture and the learning algorithms are responsible for adding inductive  biases in a trained model. For example, standard training procedures bias neural  networks towards learning “simple” classification boundaries [29], convolutional networks are location invariant [18], and MLPs ignore permutation in the input space among others. These biases manifest themselves in the representations of raw data learned by the model. While some previous work have showed the importance of invariances for purposes of generalization and optimization, in this work, we show the importance of proper inductive biases for adversarial robustness. By means of simple theoretical setups, we show how the choice of representation can drasticaly affect adversarial robustness. We also provide some experimental evidence how incorporating better inductive biases can help improve robustness." 
authors = ["Amartya Sanyal", "Varun Kanade", "Puneet K. Dokania", "Philip H.S. Torr"]
date = "2020-07-01"
math = true
draft = "false"
publication_types = ["3"]
publication = "Accepted at the NeurIPS '20 Workshop on Interpretable Inductive Biases and Physically Structured Learnin"
featured = true
title = "Choice of Representation Matters for Adversarial Robustness"
url_pdf = "https://inductive-biases.github.io/papers/33.pdf"
+++